{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARcode: HPC Application Recognition Through Image-encoded Monitoring Data\n",
    "\n",
    "This Jupyter notebook contains the necessary code for building the ARcode model for application recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import model_from_json, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, LeakyReLU\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the dataset\n",
    "DATA_PATH = Path('../dataset')\n",
    "\n",
    "# Load labels and job signatures\n",
    "labels_file_path = DATA_PATH / \"labels.npy\"\n",
    "signatures_file_path = DATA_PATH / \"signatures.npy\"\n",
    "\n",
    "labels = np.load(labels_file_path)\n",
    "signatures = np.load(signatures_file_path)\n",
    "\n",
    "# Mapping of IDs to application names. This mapping is used when creating the dataset.\n",
    "app_code = {0: \"BerkeleyGW\", 1: \"Espresso\", 2: \"Gromacs\", 3: \"LAMMPS\", 4: \"NWCHEM\", 5: \"VASP\", 6: \"WRF\", 7: \"aims\", 8: \"chroma\", 9: \"cp2k\", 10: \"e3sm\", 11: \"su3\"}\n",
    "\n",
    "# Get the input shape\n",
    "input_shape = signatures[0].shape\n",
    "\n",
    "# Labels and signatures shape\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "print(f'Sigantures shape: {signatures.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(timer()-self.starttime)\n",
    "\n",
    "cb = TimingCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to the training set (80%) and the testing set (20%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(signatures, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Change the labels from categorical to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "# Further split the training set (80%) to the training set (60%) and the validation set (20%).\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train, y_train_one_hot, test_size=0.25, random_state=42)\n",
    "\n",
    "print('Training set shape : ', X_train.shape, y_train.shape)\n",
    "print('Validation set shape : ', X_valid.shape, y_valid.shape)\n",
    "print('Testing set shape : ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "num_classes = 12\n",
    "\n",
    "weight_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "bias_initializer=tf.keras.initializers.Zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape, padding='same', kernel_regularizer=l2(0.001), kernel_initializer=weight_initializer,bias_initializer=bias_initializer))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same', kernel_regularizer=l2(0.001), kernel_initializer=weight_initializer,bias_initializer=bias_initializer))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',padding='same', kernel_regularizer=l2(0.001), kernel_initializer=weight_initializer,bias_initializer=bias_initializer))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))                  \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001), kernel_initializer=weight_initializer,bias_initializer=bias_initializer))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.7))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_train = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_valid, y_valid), callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cb.logs)\n",
    "print(sum(cb.logs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "print(f'Test loss: {test_eval[0]}')\n",
    "print(f'Test accuracy: {test_eval[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation accuracy and loss\n",
    "accuracy = signature_train.history['accuracy']\n",
    "val_accuracy = signature_train.history['val_accuracy']\n",
    "loss = signature_train.history['loss']\n",
    "val_loss = signature_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "\n",
    "plt.plot(epochs, accuracy, '-', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, '--', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, '-', label='Training loss')\n",
    "plt.plot(epochs, val_loss, '--', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict application based on the threshold value; unknown application is marked as -1.\n",
    "def threshold_prediction(prediction, threshold):\n",
    "    updated_prediction = []\n",
    "    for i in prediction:\n",
    "        max_value = max(i)\n",
    "        if max_value >= threshold:\n",
    "            index = np.where(i == max_value)\n",
    "            updated_prediction.append(index[0][0])\n",
    "        else:\n",
    "            updated_prediction.append(-1)\n",
    "    return np.array(updated_prediction)\n",
    "\n",
    "# Use 0.99 as the highest threshold value.\n",
    "thresholds = [i for i in np.arange(0, 1.00, 0.05)]\n",
    "thresholds.append(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "predicted_classes = model.predict(X_test)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = threshold_prediction(predicted_classes, threshold)\n",
    "    accu = accuracy_score(y_test, y_pred)\n",
    "    accuracy.append(accu)\n",
    "\n",
    "accuracy = [float(\"{:.4f}\".format(s)) for s in accuracy]\n",
    "\n",
    "print(f'Accuracy scores on different confidence thresholds: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Kfold to test the performance of the CNN model on identifying each application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "threshold = 0.8\n",
    "accuracy_all = []\n",
    "\n",
    "for train_idx, test_idx in k_fold.split(signatures, labels):\n",
    "    X_train = signatures[train_idx]\n",
    "    y_train = labels[train_idx]\n",
    "    \n",
    "    X_test = signatures[test_idx]\n",
    "    y_test = labels[test_idx]\n",
    "    \n",
    "    # Change the labels from categorical to one-hot encoding\n",
    "    y_train_one_hot = to_categorical(y_train)\n",
    "    y_test_one_hot = to_categorical(y_test)\n",
    "    \n",
    "    predicted_classes = model.predict(X_test)\n",
    "    y_pred = threshold_prediction(predicted_classes, threshold)\n",
    "    \n",
    "    # Get the accuracy scores\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    score = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    \n",
    "    # Formatting the scores and ignore the score of unknow application\n",
    "    score = [float(\"{:.4f}\".format(s)) for s in score[1:]]\n",
    "    \n",
    "    print(score)\n",
    "    \n",
    "    accuracy_all.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the CNN model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open(\"arcode.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "    \n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"arcode.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CNN model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load json and create model\n",
    "# json_file = open('arcode.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"arcode.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    "\n",
    "# loaded_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2699bc79b82af8f03580c6c6b0f4d6df3a98134b10e087b2f2d9e49ab46431c9"
  },
  "kernelspec": {
   "display_name": "tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
